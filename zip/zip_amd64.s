// Code generated by command: go run zip_asm.go -pkg zip -out ../zip/zip_amd64.s -stubs ../zip/zip_amd64.go. DO NOT EDIT.

#include "textflag.h"

// func sumUint64(x []uint64, y []uint64)
// Requires: AVX, AVX2
TEXT ·sumUint64(SB), NOSPLIT, $0-48
	MOVQ x_base+0(FP), CX
	MOVQ y_base+24(FP), DX
	MOVQ x_len+8(FP), AX
	MOVQ y_len+32(FP), BX
	MOVQ CX, SI
	MOVQ DX, DI
	SHLQ $0x03, AX
	ADDQ AX, SI
	MOVQ BX, AX
	SHLQ $0x03, AX
	ADDQ AX, DI
	BTL  $0x08, github·com∕segmentio∕asm∕cpu·X86+0(SB)
	JCC  x86_loop

avx2_loop:
	MOVQ    CX, AX
	MOVQ    DX, BX
	ADDQ    $0x80, AX
	ADDQ    $0x80, BX
	CMPQ    AX, SI
	JAE     x86_loop
	CMPQ    BX, DI
	JAE     x86_loop
	VMOVDQU (CX), Y0
	VMOVDQU (DX), Y1
	VMOVDQU 32(CX), Y2
	VMOVDQU 32(DX), Y3
	VMOVDQU 64(CX), Y4
	VMOVDQU 64(DX), Y5
	VMOVDQU 96(CX), Y6
	VMOVDQU 96(DX), Y7
	VPADDQ  Y0, Y1, Y0
	VPADDQ  Y2, Y3, Y2
	VPADDQ  Y4, Y5, Y4
	VPADDQ  Y6, Y7, Y6
	VMOVDQU Y0, (CX)
	VMOVDQU Y2, 32(CX)
	VMOVDQU Y4, 64(CX)
	VMOVDQU Y6, 96(CX)
	MOVQ    AX, CX
	MOVQ    BX, DX
	JMP     avx2_loop

x86_loop:
	CMPQ CX, SI
	JAE  return
	CMPQ DX, DI
	JAE  return
	MOVQ (CX), AX
	ADDQ (DX), AX
	MOVQ AX, (CX)
	ADDQ $0x08, CX
	ADDQ $0x08, DX
	JMP  x86_loop

return:
	RET
