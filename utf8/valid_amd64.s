// Code generated by command: go run valid_asm.go -pkg utf8 -out ../utf8/valid_amd64.s -stubs ../utf8/valid_amd64.go. DO NOT EDIT.

//go:build !purego
// +build !purego

#include "textflag.h"

// func validateAvx(p []byte) (bool, bool)
// Requires: AVX, AVX2, SSE2
TEXT ·validateAvx(SB), NOSPLIT, $32-26
	MOVQ p_base+0(FP), AX
	MOVQ p_len+8(FP), CX
	MOVB $0x01, DL
	BTL  $0x08, github·com∕segmentio∕asm∕cpu·X86+0(SB)
	JCS  init_avx

init_avx:
	// Prepare the constant masks
	VMOVDQU incomplete_mask<>+0(SB), Y0
	VMOVDQU cont4_vec<>+0(SB), Y1
	VMOVDQU cont3_vec<>+0(SB), Y2

	// High nibble of current byte
	VMOVDQU nibble1_errors<>+0(SB), Y3

	// Low nibble of current byte
	VMOVDQU nibble2_errors<>+0(SB), Y4

	// High nibble of the next byte
	VMOVDQU nibble3_errors<>+0(SB), Y5

	// Nibble mask
	VMOVDQU nibble_mask<>+0(SB), Y6

	// For the first pass, set the previous block as zero.
	VXORPS Y7, Y7, Y7

	// Zeroes the error vector.
	VXORPS Y8, Y8, Y8

	// Zeroes the "previous block was incomplete" vector.
	VXORPS Y9, Y9, Y9

	// Top of the loop.
check_input:
	// if bytes left >= 32
	CMPQ CX, $0x20

	// go process the next block
	JGE process

	// If < 32 bytes left
	// Fast exit if done
	CMPQ CX, $0x00
	JE   end

	// If 0 < bytes left < 32.
	// Prepare scratch space
	LEAQ    (SP), BX
	VXORPS  Y10, Y10, Y10
	VMOVDQU Y10, (BX)

	// Make a copy of the remaining bytes into the zeroed scratch space and make it the next block to read.
	MOVQ AX, SI
	MOVQ BX, AX
	CMPQ CX, $0x01
	JE   handle1
	CMPQ CX, $0x03
	JBE  handle2to3
	CMPQ CX, $0x04
	JE   handle4
	CMPQ CX, $0x08
	JB   handle5to7
	JE   handle8
	CMPQ CX, $0x10
	JBE  handle9to16
	CMPQ CX, $0x20
	JBE  handle17to32

handle1:
	MOVB (SI), CL
	MOVB CL, (AX)
	JMP  after_copy

handle2to3:
	MOVW (SI), BX
	MOVW -2(SI)(CX*1), SI
	MOVW BX, (AX)
	MOVW SI, -2(AX)(CX*1)
	JMP  after_copy

handle4:
	MOVL (SI), CX
	MOVL CX, (AX)
	JMP  after_copy

handle5to7:
	MOVL (SI), BX
	MOVL -4(SI)(CX*1), SI
	MOVL BX, (AX)
	MOVL SI, -4(AX)(CX*1)
	JMP  after_copy

handle8:
	MOVQ (SI), CX
	MOVQ CX, (AX)
	JMP  after_copy

handle9to16:
	MOVQ (SI), BX
	MOVQ -8(SI)(CX*1), SI
	MOVQ BX, (AX)
	MOVQ SI, -8(AX)(CX*1)
	JMP  after_copy

handle17to32:
	MOVOU (SI), X10
	MOVOU -16(SI)(CX*1), X11
	MOVOU X10, (AX)
	MOVOU X11, -16(AX)(CX*1)

after_copy:
	MOVQ $0x0000000000000020, CX

	// Process one 32B block of data
process:
	// Load the next block of bytes
	VMOVDQU (AX), Y10
	SUBQ    $0x20, CX
	ADDQ    $0x20, AX

	// Fast check to see if ASCII
	VPMOVMSKB Y10, BX
	CMPL      BX, $0x00
	JNZ       non_ascii

	// If this whole block is ASCII, there is nothing to do, and it is an error if any of the previous code point was incomplete.
	VPOR Y8, Y9, Y8
	JMP  check_input

non_ascii:
	XORB DL, DL

	// Check errors on the high nibble of the previous byte
	VPERM2I128 $0x03, Y7, Y10, Y9
	VPALIGNR   $0x0f, Y9, Y10, Y9
	VPSRLW     $0x04, Y9, Y11
	VPAND      Y11, Y6, Y11
	VPSHUFB    Y11, Y3, Y11

	// Check errors on the low nibble of the previous byte
	VPAND   Y9, Y6, Y9
	VPSHUFB Y9, Y4, Y9
	VPAND   Y9, Y11, Y11

	// Check errors on the high nibble on the current byte
	VPSRLW  $0x04, Y10, Y9
	VPAND   Y9, Y6, Y9
	VPSHUFB Y9, Y5, Y9
	VPAND   Y9, Y11, Y11

	// Find 3 bytes continuations
	VPERM2I128 $0x03, Y7, Y10, Y9
	VPALIGNR   $0x0e, Y9, Y10, Y9
	VPSUBUSB   Y2, Y9, Y9

	// Find 4 bytes continuations
	VPERM2I128 $0x03, Y7, Y10, Y7
	VPALIGNR   $0x0d, Y7, Y10, Y7
	VPSUBUSB   Y1, Y7, Y7

	// Combine them to have all continuations
	VPOR Y9, Y7, Y7

	// Perform a byte-sized signed comparison with zero to turn any non-zero bytes into 0xFF.
	VXORPS   Y9, Y9, Y9
	VPCMPGTB Y9, Y7, Y7

	// Find bytes that are continuations by looking at their most significant bit.
	VMOVDQU msb_mask<>+0(SB), Y9
	VPAND   Y9, Y7, Y7

	// Find mismatches between expected and actual continuation bytes
	VPXOR Y7, Y11, Y7

	// Store result in sticky error
	VPOR Y8, Y7, Y8

	// Prepare for next iteration
	VPSUBUSB Y0, Y10, Y9
	VMOVDQU  Y10, Y7

	// End of loop
	JMP check_input

end:
	// If the previous block was incomplete, this is an error.
	VPOR Y9, Y8, Y8

	// Return whether any error bit was set
	VPTEST Y8, Y8
	SETEQ  ret+24(FP)
	MOVB   DL, ret1+25(FP)
	VZEROUPPER
	RET

DATA incomplete_mask<>+0(SB)/8, $0xffffffffffffffff
DATA incomplete_mask<>+8(SB)/8, $0xffffffffffffffff
DATA incomplete_mask<>+16(SB)/8, $0xffffffffffffffff
DATA incomplete_mask<>+24(SB)/8, $0xbfdfefffffffffff
GLOBL incomplete_mask<>(SB), RODATA|NOPTR, $32

DATA cont4_vec<>+0(SB)/8, $0xefefefefefefefef
DATA cont4_vec<>+8(SB)/8, $0xefefefefefefefef
DATA cont4_vec<>+16(SB)/8, $0xefefefefefefefef
DATA cont4_vec<>+24(SB)/8, $0xefefefefefefefef
GLOBL cont4_vec<>(SB), RODATA|NOPTR, $32

DATA cont3_vec<>+0(SB)/8, $0xdfdfdfdfdfdfdfdf
DATA cont3_vec<>+8(SB)/8, $0xdfdfdfdfdfdfdfdf
DATA cont3_vec<>+16(SB)/8, $0xdfdfdfdfdfdfdfdf
DATA cont3_vec<>+24(SB)/8, $0xdfdfdfdfdfdfdfdf
GLOBL cont3_vec<>(SB), RODATA|NOPTR, $32

DATA nibble1_errors<>+0(SB)/8, $0x0202020202020202
DATA nibble1_errors<>+8(SB)/8, $0x4915012180808080
DATA nibble1_errors<>+16(SB)/8, $0x0202020202020202
DATA nibble1_errors<>+24(SB)/8, $0x4915012180808080
GLOBL nibble1_errors<>(SB), RODATA|NOPTR, $32

DATA nibble2_errors<>+0(SB)/8, $0xcbcbcb8b8383a3e7
DATA nibble2_errors<>+8(SB)/8, $0xcbcbdbcbcbcbcbcb
DATA nibble2_errors<>+16(SB)/8, $0xcbcbcb8b8383a3e7
DATA nibble2_errors<>+24(SB)/8, $0xcbcbdbcbcbcbcbcb
GLOBL nibble2_errors<>(SB), RODATA|NOPTR, $32

DATA nibble3_errors<>+0(SB)/8, $0x0101010101010101
DATA nibble3_errors<>+8(SB)/8, $0x01010101babaaee6
DATA nibble3_errors<>+16(SB)/8, $0x0101010101010101
DATA nibble3_errors<>+24(SB)/8, $0x01010101babaaee6
GLOBL nibble3_errors<>(SB), RODATA|NOPTR, $32

DATA nibble_mask<>+0(SB)/8, $0x0f0f0f0f0f0f0f0f
DATA nibble_mask<>+8(SB)/8, $0x0f0f0f0f0f0f0f0f
DATA nibble_mask<>+16(SB)/8, $0x0f0f0f0f0f0f0f0f
DATA nibble_mask<>+24(SB)/8, $0x0f0f0f0f0f0f0f0f
GLOBL nibble_mask<>(SB), RODATA|NOPTR, $32

DATA msb_mask<>+0(SB)/8, $0x8080808080808080
DATA msb_mask<>+8(SB)/8, $0x8080808080808080
DATA msb_mask<>+16(SB)/8, $0x8080808080808080
DATA msb_mask<>+24(SB)/8, $0x8080808080808080
GLOBL msb_mask<>(SB), RODATA|NOPTR, $32
